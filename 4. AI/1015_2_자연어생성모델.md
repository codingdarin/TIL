# 자연어 생성 모델 (Seq2Seq, Attention)

## 학습 목표

- 언어 모델의 개념과 역할을 설명할 수 있다
- 언어 모델이 문맥을 바탕으로 단어의 확률을 예측하는 방식을 이해한다
- Seq2Seq 구조의 기본 아이디어(인코더-디코더)를 설명할 수 있다
- Attention 메커니즘의 개념과 필요성을 설명할 수 있다

---

## 1. 언어모델이란?

### 언어모델의 정의

- 언어모델이란 인간의 두뇌가 자연어를 생성하는 능력을 모방한 모델
- 단어 시퀀스 전체에 확률을 부여하여 문장의 자연스러움을 측정한다

### 확률 계산 방식

한 문장의 확률은 각 단어의 조건부 확률들의 곱으로 표현할 수 있다:

```
P(w₁, w₂, w₃, ..., wₙ) = P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × ... × P(wₙ|w₁,w₂,...,wₙ₋₁)
```

**예시**: "the cat sat on the mat"

```
P(the cat sat on the mat) = P(the) 
                            × P(cat|the) 
                            × P(sat|the cat) 
                            × P(on|the cat sat) 
                            × P(the|the cat sat on) 
                            × P(mat|the cat sat on the)
```

이처럼 각 단어는 이전 단어들(조건부 확률)을 고려하여 등장 확률이 결정된다.

---

### N-gram 언어모델

#### N-gram의 개념

- N-gram이란 연속된 n개의 단어 묶음을 말한다
- 다양한 n-gram이 얼마나 자주 등장하는지 통계를 수집하고, 이를 활용해 다음 단어를 예측한다

#### N-gram의 종류

- **unigrams**: 단어 1개 ("The", "students", "opened", "their")
- **bigrams**: 단어 2개 ("The students", "students opened", "opened their")
- **trigrams**: 단어 3개 ("The students opened", "students opened their")
- **four-grams**: 단어 4개 ("The students opened their")

#### N-gram 확률 계산 예시

**문제**: "The students opened their ______"에서 빈칸에 들어갈 단어 예측

**4-gram 언어모델 사용 시**:

```
P(w | students opened their) = count(students opened their w) / count(students opened their)
```

**말뭉치 통계**:
- "students opened their" - 1000번 등장
- "students opened their books" - 400번 등장
  - → P(books | students opened their) = 400/1000 = 0.4
- "students opened their exams" - 100번 등장
  - → P(exams | students opened their) = 100/1000 = 0.1

따라서 "books"가 더 높은 확률로 선택됨

---

### 언어모델의 실제 사용 예시

#### 1. 일상 속의 언어모델

**스마트폰 자동완성 예시**:
- 입력: "I'll meet you at the"
- 제안 단어: "cafe", "airport", "office"
- 시스템이 문맥을 고려하여 다음 단어 후보를 제시

**검색 자동완성 예시**:
- 입력: "what is the"
- 자동완성 추천:
  - "what is the weather"
  - "what is the meaning of life"
  - "what is the dark web"
  - "what is the xfl"
  - (기타 등등)

이러한 기능들은 모두 언어모델의 확률 예측을 바탕으로 작동한다.

---

### Statistical Machine Translation (SMT)

#### SMT의 역사

- 1990년부터 2010년까지는 Machine Translation을 통계적으로 접근했다
- 예시: 한국어 → 영어 번역
  - 한국어 문장 x가 주어졌을 때, 가장 잘 맞는 영어 문장 y를 찾아야 한다
  
  ```
  argmax_y P(y|x)
  ```

#### Bayes Rule 적용

Bayes Rule을 이용해 식을 두 부분으로 쪼개어 **번역모델**과 **언어모델**로 나누는 방법을 사용했다:

```
argmax_y P(y|x) = argmax_y P(x|y)P(y)
                    ↓       ↓
                 번역모델  언어모델
```

- **번역모델 P(x|y)**: 단어와 구가 어떻게 번역되어야 하는지 모델링 → (한국어, 영어) 말뭉치로 학습
- **언어모델 P(y)**: 유창한 영어 문장을 쓰는 방법을 모델링 → 영어 데이터로 학습

---

### SMT의 한계

SMT에는 다음과 같은 한계가 존재했다:

1. **구조적 복잡성**: 시스템이 복잡하고 관리가 어려움
2. **많은 수작업**: 언어별로 별도의 feature engineering이 필요
3. **언어별 자원 구축 필요**: 각 언어에 대한 리소스 준비가 필수

→ **유지 및 확장에 어려움이 존재**

이런 한계로 인해, 이후의 기계 번역 연구는 **Neural Machine Translation (NMT)** 로 넘어가게 되었다.

---

## 2. Seq2Seq

### Neural Machine Translation (NMT)

#### NMT의 등장 배경

- 2014년 Google의 "Sequence to Sequence Learning with Neural Networks" 논문에서 처음 소개
- NMT란 **인공 신경망을 이용해 기계 번역을 수행하는 방법**
- 이때 사용되는 신경망 구조를 **Sequence-to-Sequence (Seq2Seq)** 라 하며, 두 개의 RNN으로 이루어진다

#### NMT의 의의

- 2014년 이전까지는 Statistical Machine Translation (SMT)이 주류
- NMT의 등장으로 번역 품질이 비약적으로 향상되었고, End-to-End 학습이 가능해짐

---

### Translation이 어려운 이유

#### 번역 문제의 본질

번역 문제는 **입력과 출력의 길이가 다를 수 있다**는 특징이 있다.

**예시**:
- 영어: "the black cat drank milk" (5개의 단어)
- 프랑스어: "le chat noir a bu du lait" (7개의 단어)

→ 따라서, NMT에서는 **길이가 다른 시퀀스 간의 매핑을 처리**할 수 있어야 한다.

---

### Seq2Seq의 아이디어

#### Seq2Seq 구조

Seq2Seq는 **2개의 LSTM**을 사용한다:

1. **Encoder (인코더)**
   - 한 LSTM은 입력 시퀀스를 한 타임스텝씩 읽어 고정된 차원의 큰 벡터 표현을 얻는다
   
2. **Decoder (디코더)**
   - 다른 LSTM은 앞에서 얻은 벡터로부터 출력 시퀀스를 생성한다

---

### Seq2Seq Architecture

#### 구조 설명

- Seq2Seq는 **Encoder**와 **Decoder**로 이루어진다
- **Encoder**는 입력 문장에 담긴 정보를 인코딩한다
- **Decoder**는 인코딩된 정보를 조건으로 하여 타겟 문장(출력)을 생성한다

#### 예시: 영어 → 프랑스어 번역

**입력**: `<SOS> the black cat drank milk <EOS>`  
**출력**: `<SOS> le chat noir a bu du lait <EOS>`

```
Encoder: h₁ → h₂ → h₃ → h₄ → h₅ → h₆ → h₇ → F(h₁,...,h₇)

이 최종 벡터 F(h₁,...,h₇)가 "Representation of English sentence"

Decoder: 이 벡터를 받아서 프랑스어 단어를 순차적으로 생성
         le → chat → noir → a → bu → du → lait → <EOS>
```

- `<SOS>`: Start of Sentence (문장 시작 토큰)
- `<EOS>`: End of Sentence (문장 종료 토큰)

---

### Seq2Seq의 다양한 적용

Seq2Seq 구조는 **기계번역 외에도 다양한 태스크에 적용**할 수 있다:

#### 1. 요약 (Summarization)
- 긴 길이의 문서를 읽고, 짧은 길이의 문장으로 요약된 텍스트를 출력하는 태스크

#### 2. 대화 (Dialogue)
- 사용자의 발화를 기반으로, 맥락에 맞는 대답(출력 텍스트)을 생성하는 태스크

#### 3. 코드 생성 (Code Generation)
- 자연어로 작성된 설명 혹은 명령어를 입력 받아, 그에 대응하는 프로그래밍 코드 혹은 쿼리를 출력하는 태스크

---

### Seq2Seq 학습 수행

#### End-to-End 학습

Seq2Seq 모델은 **인코더와 디코더가 하나의 통합 네트워크로 연결**되어 있다.

- 디코더에서 발생한 오차는 역전파 과정을 통해 입력을 처리한 인코더까지 전달된다
- 인코더부터 디코더까지 **전체 네트워크가 End-to-End로 동시에 최적화**된다

#### 학습 과정 시각화

```
English encoder: <SOS> → the → black → cat → drank → milk → <EOS>
                  ↓      ↓      ↓      ↓       ↓      ↓      ↓
                  h₁  →  h₂  →  h₃  →  h₄  →  h₅  →  h₆  →  h₇
                                                              ↓
                                                 [Representation vector]
                                                              ↓
French decoder:   <SOS> → le → chat → noir → a → bu → du → lait → <EOS>

역전파를 통해 인코더와 디코더가 함께 학습됨
```

---

### Seq2Seq 학습 수행 (Teacher Forcing)

#### Teacher Forcing이란?

- 학습 초반에는 모델의 예측 능력이 떨어지기 때문에 학습이 불안정할 수 있다
- **Teacher Forcing**이란, 모델이 스스로 예측한 단어 대신 **정답 단어를 디코더 입력으로 강제로 넣어줌으로써** 훨씬 안정적이고 빠르게 학습을 수행하는 방법이다

#### 예시

```
정답 출력: le chat noir a bu du lait

Teacher Forcing 적용:
Decoder 입력: <SOS> → le → chat → noir → a → bu → du → lait
Decoder 출력:  le  → chat → noir → a → bu → du → lait → <EOS>
```

빨간 박스로 표시된 정답 단어들을 디코더가 다음 단어를 예측할 때 입력으로 사용한다.

---

### Seq2Seq의 토큰 출력 방법

#### 1. Greedy Inference (탐욕적 추론)

- 토큰을 출력하는 방법 중 하나로, 각 단계에서 **가장 확률이 높은 단어를 선택**한다
- **한계**: 되돌리기가 불가능하다
  - 예시: `le ___ → le chien ___ → ...`
  - 만약 "chien"을 선택했다면, 나중에 "le → chat"이 더 나았다는 것을 알아도 돌아갈 방법이 없음

```
le → chien → ... (오답! 하지만 되돌아갈 방법이 없음...)
```

---

#### 2. Beam Search

Beam Search는 Greedy 방법의 한계를 보완하는 탐색 방법이다.

**동작 방식**:

1. 매 단계마다 k개의 가장 유망한 후보 유지
2. 후보가 `<EOS>`에 도달하면, 완성된 문장으로 리스트 추가
3. `<EOS>` 문장이 충분히 모이면 탐색 종료
4. 각 후보들의 점수를 로그 확률의 합으로 구해 최종 선택

**예시 (k=3 일 때)**:

```
시작: <SOS>
  ↓
1단계: le, chat, noir (상위 3개 선택)
  ↓
2단계: 
  - le → chat, noir, chien
  - chat → (다른 후보들)
  - noir → (다른 후보들)
  (각각 확장하여 총 9개 경로 생성 → 다시 상위 3개만 유지)
  ↓
계속 반복...
```

최종적으로 `<EOS>`까지 도달한 여러 완성 문장들 중 가장 점수가 높은 것을 선택한다.

---

## 3. Attention

### Seq2Seq의 한계: the Bottleneck Problem

#### Bottleneck Problem이란?

- 인코더는 입력 문장 전체를 하나의 벡터로 요약한다
- 마지막 hidden state에 **문장의 모든 의미 정보가 담긴다**
- 고정 길이 벡터 하나에 모든 문장의 의미를 압축하다 보니 **정보 손실이 생길 수 있다**
  - 이를 **Bottleneck Problem**이라고 한다

#### 시각화

```
English encoder: h₁ → h₂ → h₃ → h₄ → h₅ → h₆ → [h₇] ← 병목!
                                               ↓
                            인코딩 된 입력 문장의 의미 정보
                                               ↓
French decoder:                         (이 벡터만으로 디코딩)
```

h₇ 하나에 입력 문장 전체 정보가 압축되어야 하므로, 긴 문장일수록 정보 손실 위험이 크다.

---

### Attention의 인사이트

#### Attention의 핵심 아이디어

- **Attention**은 디코더가 단어를 생성할 때, **인코더 전체 hidden state 중 필요한 부분을 직접 참조**할 수 있도록 한다
- 즉, 매 타임스텝마다 **"어떤 단어/구절에 집중할지"를 가중치로 계산**해, bottleneck 문제를 완화한다

**비유**:
- 검색창에 키워드를 입력하면, 관련된 정보를 찾아주는 것처럼
- 디코더가 현재 생성할 단어와 관련된 인코더 정보를 집중적으로 가져온다

---

### Attention 메커니즘

#### Attention 동작 과정

```
English encoder: h₁ → h₂ → h₃ → h₄ → h₅ → h₆ → h₇
                  ↓    ↓    ↓    ↓    ↓    ↓    ↓
                [    Attention Score 계산    ]
                  ↓    ↓    ↓    ↓    ↓    ↓    ↓
             A(1,1) A(1,2) A(1,3) A(1,4) A(1,5) A(1,6) A(1,7)
                           ↓ (softmax)
                        [a₁ 분포]
                           ↓
                    Context vector s₁
                           ↓
French decoder:          y₁ 생성
```

#### 시각적 예시 (3개 타임스텝)

**첫 번째 출력 단어 생성 시**:
- Attention이 h₁에 집중 → s₁ 생성 → y₁ 출력

**두 번째 출력 단어 생성 시**:
- Attention이 h₂, h₃에 집중 → s₂ 생성 → y₂ 출력

**세 번째 출력 단어 생성 시**:
- Attention이 h₃에 집중 → s₃ 생성 → y₃ 출력

---

### Attention의 효과

Attention mechanism은 다음과 같은 장점이 존재한다:

#### 1. NMT 성능 향상
- 디코더가 소스 문장 전체가 아닌, **필요한 부분에만 집중**할 수 있기 때문에 번역 품질이 향상된다

#### 2. Bottleneck Problem 해결
- 디코더가 인코더의 **모든 hidden states에 직접 접근**할 수 있다
- 고정 길이 벡터 하나에 모든 정보를 압축할 필요가 없어진다

#### 3. Vanishing Gradient Problem 완화
- Attention은 멀리 떨어진 단어도 **직접 연결**할 수 있게 해준다
- 그래디언트가 소실되지 않고 잘 전달되어 학습이 안정적이다

---

### Attention 효과 시각화

#### 성능 그래프

Seq2Seq 논문의 실험 결과에 따르면:

- **RNNsearch-50** (Attention 적용): BLEU 점수가 가장 높고, 문장 길이가 길어져도 성능 저하가 적음
- **RNNenc-50** (Attention 미적용): 문장 길이가 길어질수록 BLEU 점수가 급격히 하락

**결론**: Attention을 사용하면 긴 문장에서도 성능이 안정적으로 유지된다.

#### BLEU 점수 표

| Model | All | No UNK |
|-------|-----|--------|
| RNNencdec-30 | 13.93 | 24.19 |
| RNNsearch-30 | 21.50 | 31.44 |
| RNNencdec-50 | 17.82 | 26.71 |
| RNNsearch-50 | 26.75 | 34.16 |
| RNNsearch-50* | 28.45 | 36.15 |
| Moses (전통적 SMT) | 33.30 | 35.63 |

- RNNsearch (Attention 적용 모델)이 RNNenc (Attention 미적용) 대비 월등히 높은 성능
- RNNsearch-50*은 더 오래 학습시킨 버전

---

### Attention의 효과: 해석 가능성 (Interpretability)

#### 모델 내부 이해 가능

- Attention 분포를 보면, **decoder가 어떤 단어를 생성할 때, 입력 문장의 어느 부분에 집중했는지** 확인할 수 있다
- 즉, 모델이 내부적으로 참고한 근거를 사람이 파악할 수 있음
- → **모델의 의사결정 과정을 해석**할 수 있는 단서

---

### Attention의 효과: 정렬 (Alignment)

#### 자동 단어 정렬

- 기계번역에서는 전통적으로 단어 alignment 모델을 따로 학습해야 했다
- 하지만, **attention은 통해 decoder가 필요한 입력 단어에 자동으로 집중**하기 때문에, 단어와 단어 간의 매핑 관계를 자연스럽게 학습한다

#### Alignment 히트맵 예시

```
입력 (영어):     The agreement on the European Economic Area
출력 (프랑스어):  L'  accord sur  la  zone  économique  européenne

Attention 히트맵:
- "L'" → "The"에 집중
- "accord" → "agreement"에 집중
- "européenne" → "European"에 집중
- "économique" → "Economic"에 집중
```

이처럼 Attention은 입력-출력 간 단어 정렬을 자동으로 학습한다.

---

### Attention: Query와 Values

#### Query와 Values의 관계

Seq2Seq에서 attention을 사용할 때, **각 decoder의 hidden state**와 **모든 encoder의 hidden states** 간의 관계를 다음과 같이 볼 수 있다:

- **Query**: 각 decoder의 hidden state (현재 생성 중인 단어 정보)
- **Values**: 모든 encoder의 hidden states (입력 문장 정보)

#### Attention 과정 정리

1. **Query와 Values 사이 유사도 점수(score) 계산**
   - 예: dot-product, multiplicative, additive 등
   
2. **Softmax를 통해 확률 분포(attention distribution) 얻기**
   - 어떤 입력 단어에 얼마나 집중할지 비율로 표현
   
3. **분포를 이용해 values를 가중합 → context vector (attention output) 생성**

#### 수식 비교

**Dot product**:
```
h · s
```

**Multiplicative**:
```
h · W · s
```

**Additive**:
```
v · tanh(Wh·h + Ws·s)
```

각 방법은 Query와 Values 간 유사도를 계산하는 방식이 다르다.

---

## 요약

### 언어모델
- 인간의 두뇌가 자연어를 생성하는 능력을 모방
- 단어 시퀀스에 확률 부여, 조건부 확률의 곱으로 문장 확률 계산
- N-gram 모델: 통계 기반, 하지만 한계 존재
- Neural Machine Translation으로 발전

### Seq2Seq
- Encoder-Decoder 구조
- Encoder: 입력 시퀀스를 고정 길이 벡터로 인코딩
- Decoder: 인코딩된 벡터로부터 출력 시퀀스 생성
- Teacher Forcing: 학습 안정화 기법
- Greedy/Beam Search: 토큰 출력 방법
- 다양한 응용: 번역, 요약, 대화, 코드 생성 등

### Attention
- Bottleneck Problem 해결
- 디코더가 인코더의 모든 hidden states에 직접 접근
- 매 타임스텝마다 중요한 부분에 집중
- 효과:
  1. NMT 성능 향상
  2. Bottleneck Problem 해결
  3. Vanishing Gradient Problem 완화
  4. 해석 가능성 (Interpretability)
  5. 자동 정렬 (Alignment)
- Query-Values 관계로 attention 계산
