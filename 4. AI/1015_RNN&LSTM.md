# 워드 임베딩과 순환신경망 기반 모델 (RNN & LSTM)

## 학습 목표

- 단어를 숫자로 표현하는 기본 아이디어를 이해
- 언어에서 '순서(문맥)'의 중요성을 설명
- RNN의 기본 구조와 역할을 이해
- LSTM의 핵심 아이디어를 설명

---

## 0. 학습 시작 (오버뷰)

### 언어를 어떻게 숫자로 표현할까?

- One-hot encoding, word embedding 등 기본 아이디어를 이해

### 언어에서 '순서(문맥)'는 왜 중요하고 어떻게 다뤄야 할까?

- 단어 순서와 의미의 연결성을 파악

### 순환 신경망(RNNs, LSTMs 등)은 어떻게 동작할까?

- 기본 구조와 역할
- 장기 의존성 문제와 LSTM의 핵심 아이디어

---

## 1. 워드 임베딩

### 1-1. 원-핫 인코딩 (One-hot Encoding)

#### 원-핫 인코딩이란?

- 규칙 기반 혹은 통계적 자연어처리 연구의 대다수는 단어를 원자적(쪼갤 수 없는) 기호로 취급
  - 예: *hotel, conference, walk* 같은 단어들
- 벡터 공간 관점에서 보면, 이는 한 원소만 1이고 나머지는 모두 0인 벡터를 의미

```
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]
```

- 차원 수(= 단어 사전 크기)는 대략 다음과 같음:
  - 음성 데이터(2만) - Penn Treebank (PTB) 코퍼스(5만) - big vocab(50만) - Google 1T(1300만)
- 이를 **원-핫(one-hot) 표현**이라고 부르고, 단어를 원-핫 표현으로 바꾸는 과정을 **원-핫 인코딩(one-hot encoding)**이라고 함

#### 원-핫 인코딩의 문제점

**예시: 웹 검색**

- [삼성 노트북 배터리 사이즈] == [삼성 노트북 배터리 용량]
- [갤럭시 핸드폰] == [갤럭시 스마트폰]

```
핸드폰 = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]ᵀ
스마트폰 = [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0] = 0
```

- 검색 쿼리 벡터와 대상이 되는 문서 벡터들이 서로 **직교**하게 되어, 원-핫 벡터로는 유사도를 측정할 수 없음

#### 원-핫 인코딩과 같은 전통적인 텍스트 표현 방식의 여러 한계

**1. 차원의 저주(Curse of Dimensionality)**

- 고차원의 희소 벡터를 다루기 위해선 많은 메모리가 필요
- 차원이 커질수록 데이터가 점점 더 희소(sparse)해져 활용이 어려움

**2. 의미적 정보 부족**

- 비슷한 단어라도 유사한 벡터로 표현되지 않음
- 예시: "은행"과 "금융"은 의미적으로 밀접하지만, 원-핫 인코딩에서는 전혀 무관한 벡터로 취급됨

---

### 1-2. 워드 임베딩 (Word Embedding)

#### 주변 단어를 활용해보기

- 단어를 주변 단어들로 표현하면, 많은 의미를 담을 수 있음

**"You shall know a word by the company it keeps"**  
(distributional hypothesis)

- 현대 통계적 자연어처리에서의 가장 성공적인 아이디어 중 하나

정부는 가계부채 문제를 해결하기 위해 **은행** 대출 규제를 강화했다.

인공지능 기반 챗봇이 **은행** 상담 서비스에 도입되었다.

→ 이런 주변 단어들로 **은행**의 의미를 나타낼 수 있음

#### 워드 임베딩이란?

- 단어를 단어들 사이의 의미적 관계를 포착할 수 있는 **밀집(dense)되고 연속적/분산적(distributed) 벡터 표현**으로 나타내는 방법
  - 원-핫 인코딩에선 은행과 금융이 완전히 독립적(무관한) 벡터로 표현되었지만,
  - 워드 임베딩에선 두 단어의 벡터가 공간상 서로 가깝게 위치하며, 이를 통해 의미적 유사성을 반영할 수 있음

```
은행 = [0.432, 0.343, -0.324, 0.743, -0.853, 0.135, 0.738, ...]
       ↓
       정부, 위기, 규제, 돈 등 예측 가능
```

---

#### 대표적인 워드 임베딩 기법 - Word2Vec

- Word2Vec은 2013년 Google에서 개발한 워드 임베딩 기법
- 단어의 표현을 간단한 인공 신경망을 이용해서 학습

**Word2Vec에 두 가지 알고리즘이 존재:**

**1) Skip-grams (SG) 방식**

- 중심 단어를 통해 주변 단어들을 예측하는 방법
- 단어의 위치(앞/뒤)에 크게 구애 받지 않음

**2) Continuous Bag of Words (CBOW) 방식**

- 주변 단어들을 통해 중심 단어를 예측하는 방법
- 문맥 단어들의 집합으로 중심 단어를 맞춤

---

#### Skip-grams (SG): 중심 단어를 통해 주변 단어 예측하기

- 윈도우 크기(window size) = 중심 단어 주변 몇 개 단어를 문맥으로 볼 것인가?

**예시 (윈도우 크기 = 2)**

- 문장: "… problems turning into banking crises as …"
- 중심 단어 "banking" (위치 t)
- 주변 단어 = {"turning", "into", "crises", "as"}

```
P(w_{t-2} | w_t)    P(w_{t+2} | w_t)
       ↓                   ↓
   P(w_{t-1} | w_t)   P(w_{t+1} | w_t)
          ↓               ↓
... problems turning into banking crises as ...
    |------------|  |위치 t|  |------------|
    주변 단어들      중심 단어    주변 단어들
   (윈도우 크기 2)            (윈도우 크기 2)
```

---

#### Continuous Bag of Words (CBOW): 주변 단어를 통해 중심 단어 예측하기

- 목표: 주변 단어들의 집합이 주어졌을 때, 그 문맥과 함께 등장할 수 있는 단일 단어를 예측

**CBOW와 Skip-gram 구조 비교**

```
        INPUT    PROJECTION    OUTPUT
w(t-2)    □
           \
w(t-1)    □  \      
              SUM  →   □   →   w(t)
w(t+1)    □  /
           /
w(t+2)    □

           CBOW
```

```
        INPUT    PROJECTION    OUTPUT
                                  □  w(t-2)
                                 /
                                □  w(t-1)
w(t)      □   →    □   →       
                                \
                                 □  w(t+1)
                                  \
                                   □  w(t+2)

                              Skip-gram
```

---

#### CBOW 예시

- 목표: 주번 단어들의 집합이 주어졌을 때, 그 문맥과 함께 등장할 수 있는 단일 단어를 예측

**문장 예시:**

```
The  [quick] [brown] [fox]  runs  away.
     |------ 윈도우 ------|
```

```
         INPUT    PROJECTION    OUTPUT

The        →      w(t-2)
                     \
quick      →      w(t-1)  \
                          SUM  →  w(t)  →  brown
fox        →      w(t+1)  /
                     /
runs       →      w(t+2)
```

---

#### Skip-Gram vs. CBOW 비교

| 모델 | 장점 | 한계 |
|------|------|------|
| Skip-Gram | - 적은 데이터에도 잘 동작<br>- 희귀 단어나 구 표현에 강함 | - 학습 속도가 느림 |
| CBOW | - 학습 속도가 빠름<br>- 자주 나오는 단어에 강함 | - 희귀 단어 표현에 약함 |

**One CBOW example:**

```
yesterday was a really [...] day  →  Beautiful?
                                      Nice?
                                      Delightful?
```

**Five Skip-gram examples:**

```
Delightful  →  yesterday was a really [...] day
           ↘
            yesterday was a really [...] day
```

---

---

## 2. 순차 데이터와 언어의 순서

### 2-1. 왜 순서가 중요한가?

#### 순서에 따른 의미 변화

- 같은 단어들이라도 순서에 따라 의미가 완전히 달라짐
  - "개가 사람을 물었다" ≠ "사람이 개를 물었다"
  - "나는 학교에 간다" vs "학교에 나는 간다" vs "간다 학교에 나는" (비문)

#### Word2Vec의 한계

- Word2Vec은 단어 하나당 벡터 하나를 할당
- 같은 단어는 문맥과 무관하게 항상 같은 벡터
  - "The bank by the river" (강둑)
  - "The bank raised rates" (은행)
  - 두 "bank"가 같은 벡터로 표현됨 → 문맥 정보 손실

#### 순차 데이터(Sequential Data)의 특징

- 시간 순서가 중요한 데이터
  - 자연어: 단어의 순서
  - 시계열: 주가, 온도, 센서 데이터
  - 음성/음악: 소리의 순서
  - 비디오: 프레임의 순서

---

### 2-2. 기존 신경망의 한계

#### Feedforward Neural Network의 문제

- 입력을 한꺼번에 처리 → 순서 정보 손실
- 입력 길이가 고정되어야 함
  - "나는 학교에 간다" (4단어) ✓
  - "나는 오늘 아침 일찍 학교에 걸어서 간다" (9단어) ✗
- 가변 길이 입력 처리 불가능

---

## 3. RNN (Recurrent Neural Network)

### 3-1. RNN의 핵심 아이디어

#### 기본 개념

- **순서대로 하나씩 처리**: 단어를 시간 순서대로 입력
- **이전 정보를 기억**: 은닉 상태(hidden state)에 과거 정보 저장
- **가변 길이 처리**: 길이에 관계없이 처리 가능

#### RNN의 구조

```
시각 t=0     t=1        t=2        t=3
         
  x₀        x₁         x₂         x₃
   ↓         ↓          ↓          ↓
 [RNN] → [RNN] →  [RNN] →  [RNN]
   ↓         ↓          ↓          ↓
  h₀  →    h₁   →     h₂   →     h₃
```

- **x**: 입력 (단어)
- **h**: 은닉 상태 (hidden state) = 기억 메모리
- **y**: 출력 (예측)

---

### 3-2. RNN의 동작 원리

#### 수식

```
hₜ = tanh(Wₕ · hₜ₋₁ + Wₓ · xₜ + b)
     └─────┬─────┘   └───┬───┘
       이전 기억      현재 입력
```

#### 예시: "나는 학교에 간다"

```
t=1: "나는"
  h₁ = f(x₁, h₀)  ← h₀는 초기값 (보통 0)
  
t=2: "학교에"
  h₂ = f(x₂, h₁)  ← h₁에 "나는" 정보가 담겨있음
  
t=3: "간다"
  h₃ = f(x₃, h₂)  ← h₂에 "나는 학교에" 정보가 담겨있음
```

---

### 3-3. RNN의 장단점

#### 장점

1. **가변 길이 입력 처리**: 짧은 문장, 긴 문장 모두 처리 가능
2. **문맥 정보 활용**: 이전 단어들의 정보를 활용하여 현재 단어 처리
3. **파라미터 공유**: 같은 가중치를 모든 시간 단계에서 재사용 → 메모리 효율적

#### 단점: 장기 의존성 문제

**기울기 소실(Vanishing Gradient)**

- 짧은 문장: "나는 학교에 간다" (4단어) → 잘 동작
- 긴 문장: "나는 어제 ... (20개 단어) ... 간다" → 초반 정보 손실

```
문제 원인:
tanh 함수의 미분값: 0 ~ 1
여러 번 곱하면: 0.9 × 0.8 × ... (20번) ≈ 0.00000001

→ 초반 단어의 기울기가 거의 0에 가까워짐
→ 가중치 업데이트 안됨
→ 장기 정보 학습 실패
```

---

## 4. LSTM (Long Short-Term Memory)

### 4-1. LSTM의 핵심 아이디어

**"중요한 정보는 오래 기억하고, 안 중요한 정보는 빨리 잊자"**

#### RNN vs LSTM의 차이

**RNN:**
```
h → h → h → h  (매번 변형, 정보 손실)
```

**LSTM:**
```
C ─────────────→ C  (거의 그대로 전달, 정보 보존)
│                │
h                h  (필요한 것만 출력)
```

- **Cell State (C)**: 장기 기억 저장소 (내부 전용)
- **Hidden State (h)**: 현재 작업 메모리 (외부 출력용)

---

### 4-2. LSTM의 구조

#### 4개의 주요 구성요소

1. **Forget Gate (망각 게이트)**: 뭘 잊을까?
2. **Input Gate (입력 게이트)**: 뭘 저장할까?
3. **Cell State Update**: Cell State 업데이트
4. **Output Gate (출력 게이트)**: 뭘 출력할까?

```
       Cₜ₋₁ ─────────────────→ Cₜ
        │    (컨베이어 벨트)     │
        │                       │
     [Forget] [Input]        [Output]
        ↓       ↓               ↓
       xₜ, hₜ₋₁             →  hₜ
```

---

### 4-3. LSTM의 게이트 메커니즘

#### 1. Forget Gate (망각 게이트)

**"이전 Cell State 중에서 뭘 잊을까?"**

```
fₜ = σ(Wf·[hₜ₋₁, xₜ] + bf)

σ = sigmoid 함수 (출력: 0~1)
fₜ = [0.1, 0.9, 0.05, 0.95, ...]
     │    │    │     │
    거의  완전  거의  완전
    잊음  기억  잊음  기억
```

**역할**: Sigmoid로 0~1 값 생성 → 각 정보를 얼마나 기억할지 결정

---

#### 2. Input Gate (입력 게이트)

**"새로운 정보 중에서 뭘 저장할까?"**

```
iₜ = σ(Wi·[hₜ₋₁, xₜ] + bi)         # 저장 여부 (0~1)
C̃ₜ = tanh(Wc·[hₜ₋₁, xₜ] + bc)     # 후보 값 (-1~1)
```

- **iₜ**: "얼마나 저장할지" 결정 (Sigmoid)
- **C̃ₜ** (C tilde): "무엇을 저장할지" 내용 (Tanh)
  - 틸다(~) 기호: 후보, 제안 값을 의미
  - 실제 저장량 = iₜ ⊙ C̃ₜ

**Sigmoid vs Tanh 역할 분담**

| 함수 | 범위 | 역할 | 사용 위치 |
|------|------|------|-----------|
| Sigmoid | 0~1 | **얼마나** (게이트) | fₜ, iₜ, oₜ |
| Tanh | -1~1 | **무엇을** (내용) | C̃ₜ, tanh(Cₜ) |

---

#### 3. Cell State 업데이트

**"과거 기억 + 새 정보"**

```
Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ
    └────┬────┘   └────┬────┘
      이전 기억       새 정보
       망각          추가
```

**⊙ = Element-wise Multiplication (원소별 곱셈)**

- 같은 위치의 원소끼리 곱셈
- 차원 유지
- 각 차원을 독립적으로 제어

```
예시:
Cₜ₋₁ = [0.8, 0.6, -0.3, 0.5]
fₜ   = [0.1, 0.9, 0.8, 0.95]

fₜ ⊙ Cₜ₋₁ = [0.08, 0.54, -0.24, 0.475]
            각 정보를 독립적으로 제어!
```

---

#### 4. Output Gate (출력 게이트)

**"Cell State 중에서 뭘 밖으로 내보낼까?"**

```
oₜ = σ(Wo·[hₜ₋₁, xₜ] + bo)
hₜ = oₜ ⊙ tanh(Cₜ)
```

- Cell State에서 필요한 정보만 선택적으로 출력
- 전체를 출력하지 않고 필터링

---

### 4-4. Cell State vs Hidden State

#### 역할 비교

| 구분 | Cell State (C) | Hidden State (h) |
|------|---------------|------------------|
| **역할** | 장기 정보 저장소 | 현재 작업용 정보 |
| **변화** | 느림 (덧셈 위주) | 빠름 (매번 새로 계산) |
| **범위** | 과거~현재 누적 | 현재 시점 중심 |
| **출력** | 내부 전용 | 외부로 출력됨 |
| **용도** | 장기 의존성 해결 | 예측, 다음 레이어 전달 |

#### 관계

- **C = 전체 정보 저장 (냉장고)**
- **h = C에서 필요한 것만 선택 (도마 위 재료)**
- h는 C의 부분집합 (h ⊂ C)
- "중첩"이 아니라 "선택적 추출"

```
C = [프랑스:0.9, 유럽:0.8, 나라:0.7, ...]  ← 모든 정보
o = [0.1, 0.2, 0.9, ...]  ← 선택 필터
h = [프랑스:0.09, 유럽:0.16, 나라:0.63, ...]  ← 필요한 것만
```

---

### 4-5. LSTM이 RNN보다 나은 이유

#### 정보 전달 방식의 차이

**RNN: 곱셈 위주**
```
h = tanh(W·h + ...)
    └─── 매번 곱셈/변환

0.9 × 0.9 × ... (20번) ≈ 0.12  → 정보 손실
```

**LSTM: 덧셈 위주**
```
Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ
    └─── 덧셈 (정보 보존)

중요한 정보는 fₜ=1로 설정
→ Cₜ = 1 ⊙ Cₜ₋₁ + ... = Cₜ₋₁ + ...
→ 거의 그대로 전달!
```

---

### 4-6. LSTM 동작 예시

**문장: "프랑스는 유럽에 있다. 그 나라의 수도는 파리다."**

```
t=1: "프랑스는"
C₁ = [프랑스:0.9, ...]  (저장)
h₁ = [프랑스:0.8, ...]  (출력)

t=2: "유럽에"
f₂ = [0.9, ...]  (프랑스 정보 유지)
C₂ = [프랑스:0.88, 유럽:0.85, ...]  (누적)
h₂ = [유럽:0.9, ...]  (현재는 유럽만 출력)

t=8: "파리다"
C₈ = [프랑스:0.75, 파리:0.95, ...]  (프랑스 정보 여전히 존재!)
h₈ = [파리:0.9, ...]  (파리 출력)

→ 7단계 전 "프랑스" 정보가 C에 보존되어 있음!
```

---

## 5. 핵심 개념 정리

### 차원의 변화

```
단어 "은행"
  ↓
원-핫 인코딩: 50,000차원 (희소 벡터)
  ↓
워드 임베딩: 300차원 (밀집 벡터, 의미 포함)
  ↓
RNN 은닉상태: 100차원 (문맥 반영)
```

### 워드 임베딩

- **원-핫 인코딩**: 고차원 희소 벡터, 의미 관계 X
- **워드 임베딩**: 저차원 밀집 벡터, 의미 관계 O
- **Word2Vec**: 주변 단어로 의미 학습
  - **Skip-Gram**: 중심 단어 → 주변 단어 예측 (희귀 단어에 강함)
  - **CBOW**: 주변 단어 → 중심 단어 예측 (빠름)

### RNN

- **핵심**: 순차 처리, 은닉 상태로 이전 정보 기억
- **수식**: hₜ = tanh(Wₕ·hₜ₋₁ + Wₓ·xₜ)
- **장점**: 가변 길이, 문맥 활용, 파라미터 공유
- **단점**: 장기 의존성 문제 (기울기 소실)

### LSTM

- **핵심**: Cell State로 장기 정보 보존
- **구조**: 4개 게이트 (Forget, Input, Update, Output)
- **Cell State (C)**: 장기 기억 저장소 (내부 전용)
- **Hidden State (h)**: 현재 작업 메모리 (외부 출력)
- **장점**: 장기 의존성 해결, 선택적 기억/망각

### 주요 수학 기호

- **σ (Sigmoid)**: 0~1 범위, 게이트 역할 (얼마나)
- **tanh**: -1~1 범위, 내용 표현 (무엇을)
- **⊙**: Element-wise 곱셈 (원소별 곱셈, 차원 유지)
- **C̃ (C tilde)**: 후보 값, 제안 값